{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1gyiNmrqs2TpX2dXQH0ZCnfgqSa2sxuo-","authorship_tag":"ABX9TyMTvgN/8Zrpn+5UM027Wn7p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nMnQdRqcC_fa","executionInfo":{"status":"ok","timestamp":1739114916020,"user_tz":-330,"elapsed":25924,"user":{"displayName":"Shyamala Akshitha","userId":"08931687402385668466"}},"outputId":"2f288eea-a223-452f-894f-d06408201141"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai-whisper\n","  Downloading openai-whisper-20240930.tar.gz (800 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m645.1/800.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.67.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n","Collecting tiktoken (from openai-whisper)\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Collecting triton>=2.0.0 (from openai-whisper)\n","  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n","Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803373 sha256=5f775b62e713b1e0d39203f7b280af29a415e6a4c2d4d43e3903ea013814ca77\n","  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n","Successfully built openai-whisper\n","Installing collected packages: triton, tiktoken, openai-whisper\n","Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.2.0\n"]}],"source":["pip install -U openai-whisper"]},{"cell_type":"code","source":["!whisper \"small.mp3\" --model medium.en"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bzwrwYCnVjAt","executionInfo":{"status":"ok","timestamp":1739116433877,"user_tz":-330,"elapsed":1137708,"user":{"displayName":"Shyamala Akshitha","userId":"08931687402385668466"}},"outputId":"69ae4b0b-4dac-4c75-88be-f777510c96ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100%|█████████████████████████████████████| 1.42G/1.42G [00:30<00:00, 50.2MiB/s]\n","/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(fp, map_location=device)\n","/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","[00:00.000 --> 00:05.840]  Over on the left hand side, let's click on this folder icon and you can now drag in an audio file\n","[00:05.840 --> 00:11.920]  or a video file that you would like to transcribe. Here I have an MP3 file and I'll simply drop this\n","[00:11.920 --> 00:17.360]  in. Here it says that the uploaded files will get deleted when this runtime is recycled. That's okay,\n","[00:17.360 --> 00:22.800]  so let's click on okay. And now we can see that the file has been successfully uploaded.\n","[00:22.800 --> 00:28.880]  I'm now ready to extract text from this audio file. Let's go back up to the top and here I'll\n","[00:28.880 --> 00:34.320]  insert some code. This inserts another field down below and here I'll type in whisper. Here this is\n","[00:34.320 --> 00:40.400]  calling the whisper AI. Then you need to type in the name of the file that you want to extract\n","[00:40.400 --> 00:47.360]  text from. Mine is called cookies.mp3, so here I'll make sure it says cookies.mp3. And last,\n","[00:47.360 --> 00:54.160]  you can also specify the model that you would like to use. I want to use the medium model. You have\n","[00:54.160 --> 00:59.520]  five different models that you can choose from. On the low end, you have the tiny model. This\n","[00:59.520 --> 01:04.800]  takes up the least space. It also works the quickest, but you get the worst accuracy.\n","[01:05.600 --> 01:11.200]  On the other end, you have the large model. It takes up about a gig and a half. It also takes\n","[01:11.200 --> 01:17.200]  the longest time to process, but you also get the highest quality level. I found that a good sweet\n","[01:17.200 --> 01:20.960]  spot is going with the medium model. Once you finish entering this in.\n"]}]},{"cell_type":"markdown","source":["Create a Python script that processes a directory of media files by:\n","\n","Folder Parsing: Recursively scanning the provided folder (including subfolders) to locate any audio or video files.\n","Transcription: Using the Whisper package with the smallest available model to transcribe each media file.\n","Saving Results: Storing each transcription in a structured format (e.g., text or JSON) alongside the original files or in a designated output folder."],"metadata":{"id":"3dbJDfgThM-c"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"qkYCBtVT5Pj7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739119250807,"user_tz":-330,"elapsed":65732,"user":{"displayName":"Shyamala Akshitha","userId":"08931687402385668466"}},"outputId":"5ccec060-51ff-4dcf-e273-874952e5e2bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the input directory path: /content/drive/MyDrive/Colab Notebook_2/input_media\n","Enter output directory path: /content/drive/MyDrive/Colab Notebook_2/output\n","Loading Whisper model...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(fp, map_location=device)\n"]},{"output_type":"stream","name":"stdout","text":["Scanning for media files...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"," 50%|█████     | 1/2 [00:27<00:27, 27.36s/it]/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","100%|██████████| 2/2 [00:53<00:00, 26.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Transcription complete!\n","Transcription complete!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import os\n","import json\n","import whisper\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","\n","def finding_files(directory):\n","    # Finding audio files in directory\n","    extensionsofmedia = ('.mp3', '.wav', '.m4a', '.mp4', '.avi', '.mov')\n","    return [os.path.join(root,file) for root,_,files in os.walk(directory) for file in files if file.lower().endswith(extensionsofmedia)]\n","\n","def transcribe_file(model, file_path):\n","    # Transcribe a single media file using Whisper\n","    return model.transcribe(file_path)[\"text\"]\n","\n","def save_transcription(file_path, transcription, output_dir=None):\n","    # Save transcription result to a file\n","    original_path = os.path.basename(file_path)\n","    output_name = os.path.splitext(original_path)[0] + \"\"\n","    output_path = os.path.join(output_dir, output_name)\n","\n","    # Save as both text and JSON\n","    with open(f\"{output_path}.txt\", \"w\", encoding=\"utf-8\") as f:\n","        f.write(transcription)\n","\n","    with open(f\"{output_path}.json\", \"w\", encoding=\"utf-8\") as f:\n","        json.dump({\n","            \"original_file\": str(original_path),\n","            \"transcription\": transcription\n","        }, f, indent=2)\n","\n","def main():\n","    # directory from user\n","    input_directory = input(\"Enter the input directory path: \").strip()\n","    output_directory = input(\"Enter output directory path: \").strip()\n","\n","    # Load the smallest Whisper model\n","    print(\"Loading Whisper model...\")\n","    model = whisper.load_model(\"tiny\")\n","\n","    # Find all media files\n","    print(\"Scanning for media files...\")\n","    media_files = finding_files(input_directory)\n","\n","    # Process each file\n","    for file_path in tqdm(media_files):\n","        transcription = transcribe_file(model, file_path)\n","        save_transcription(file_path, transcription, output_directory)\n","\n","    print(\"Transcription complete!\")\n","\n","    # # Process each file without tqdm\n","    # for file_path in media_files:\n","    #   print(f\"Processing: {os.path.basename(file_path)}\")\n","    #   transcription = transcribe_file(model, file_path)\n","    #   save_transcription(file_path, transcription, output_directory)\n","\n","    print(\"Transcription complete!\")\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}